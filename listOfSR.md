# A list of SR which should I read new.
## ICCV 2021
### 1.Pesavento, Marco, Marco Volino, and Adrian Hilton. "Attention-based Multi-Reference Learning for Image Super-Resolution." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
- abstract:This paper proposes a novel Attention-based Multi- Reference Super-resolution network (AMRSR) that, given a low-resolution image, learns to adaptively transfer the most similar texture from multiple reference images to the super-resolution output whilst maintaining spatial coher- ence. The use of multiple reference images together with attention-based sampling is demonstrated to achieve sig- nificantly improved performance over state-of-the-art ref- erence super-resolution approaches on multiple benchmark datasets. Reference super-resolution approaches have re- cently been proposed to overcome the ill-posed problem of image super-resolution by providing additional information from a high-resolution reference image. Multi-reference super-resolution extends this approach by providing a more diverse pool of image features to overcome the inherent in- formation deficit whilst maintaining memory efficiency. A novel hierarchical attention-based sampling approach is introduced to learn the similarity between low-resolution image features and multiple reference images based on a perceptual loss. Ablation demonstrates the contribution of both multi-reference and hierarchical attention-based sam- pling to overall performance. Perceptual and quantitative ground-truth evaluation demonstrates significant improve- ment in performance even when the reference images devi- ate significantly from the target image. The project website can be found at https://marcopesavento.github.io/AMRSR/
- Generalisation of single reference super-resolution to multiple reference images whilst improving memory efficiency thanks to a part-based mechanism.
- Hierarchical attention-based adaptive sampling for perceptual similarity learning between low-resolution image features and multiple HR reference images.
- Improved quantitative and perceptual performance for image super-resolution compared with state-of-the-art single-image RefSR.
### 2.Wang, Longguang, et al. "Learning a single network for scale-arbitrary super-resolution." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
- abstract:Recently, the performance of single image super- resolution (SR) has been significantly improved with pow- erful networks. However, these networks are developed for image SR with specific integer scale factors (e.g., ×2/3/4), and cannot handle non-integer and asymmetric SR. In this paper, we propose to learn a scale-arbitrary image SR net- work from scale-specific networks. Specifically, we develop a plug-in module for existing SR networks to perform scale- arbitrary SR, which consists of multiple scale-aware feature adaption blocks and a scale-aware upsampling layer. More- over, conditional convolution is used in our plug-in module to generate dynamic scale-aware filters, which enables our network to adapt to arbitrary scale factors. Our plug-in module can be easily adapted to existing networks to real- ize scale-arbitrary SR with a single model. These networks plugged with our module can produce promising results for non-integer and asymmetric SR while maintaining state-of- the-art performance for SR with integer scale factors. Be- sides, the additional computational and memory cost of our module is very small.
- 1) We develop a plug-in module for existing SR networks to achieve scale-arbitrary SR, including multiple scale-aware feature adaption blocks and a scale-aware upsampling layer. 2) Our plug-in module uses conditional convolution to dy- namically generate filters based on the input scale infor- mation, which facilitates our network to adapt to specific scale factors. 3) Experimental results show that baseline networks equipped with our module produce promising re- sults for scale-arbitrary SR with only a single model. A video demo is provided in the supplemental material. 
## CVPR 2021
### 1.Hui, Zheng, et al. "Learning the Non-Differentiable Optimization for Blind Super-Resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:Previous convolutional neural network (CNN) based blind super-resolution (SR) methods usually adopt an it- erative optimization way to approximate the ground-truth (GT) step-by-step. This solution always involves more com- putational costs to bring about time-consuming inference. At present, most blind SR algorithms are dedicated to ob- taining high-fidelity results; their loss function generally employs L1 loss. To further improve the visual quality of SR results, perceptual metric, such as NIQE, is necessary to guide the network optimization. However, due to the non-differentiable property of NIQE, it cannot be as the loss function. Towards these issues, we propose an adap- tive modulation network (AMNet) for multiple degradations SR, which is composed of the pivotal adaptive modulation layer (AMLayer). It is an efficient yet lightweight fusion layer between blur kernel and image features. Equipped with the blur kernel predictor, we naturally upgrade the AMNet to the blind SR model. Instead of considering it- erative strategy, we make the blur kernel predictor train- able in the whole blind SR model, in which AMNet is well- trained. Also, we fit deep reinforcement learning into the blind SR model (AMNet-RL) to tackle the non-differentiable optimization problem. Specifically, the blur kernel predic- tor will be the actor to estimate the blur kernel from the input low-resolution (LR) image. The reward is designed by the pre-defined differentiable or non-differentiable metric. Extensive experiments show that our model can outperform state-of-the-art methods in both fidelity and perceptual met- rics.
- We design a novel modified AdaIN module, which can be used in our proposed adaptive modulation network (AMNet) to better fulfill the multiple degradations SR problem while having the attributes of lower computa- tional cost and higher speed than the previous multiple degradations SR methods [35, 10, 30]. To pursue the perceptual effect, we also construct a GAN-based ver- sion of AMNet, denoted as AMGAN.
- We introduce an efficient RL algorithm into our whole blind SR framework. It can optimize the policy to ac- complish the blur kernel estimation task guided by the non-differentiable evaluation metrics. To the best of our knowledge, the proposed method is the first RL that optimizes blind SR with the in-differentiable per- ceptual metrics.
- We validate our AMNet-RL (PSNR-oriented), and AMGAN-RL (perception-driven) can achieve compa- rable results on commonly used datasets.
### 2.Liang, Jingyun, et al. "Flow-based Kernel Prior with Application to Blind Super-Resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:Kernel estimation is generally one of the key problems for blind image super-resolution (SR). Recently, Double- DIP proposes to model the kernel via a network architec- ture prior, while KernelGAN employs the deep linear net- work and several regularization losses to constrain the ker- nel space. However, they fail to fully exploit the general SR kernel assumption that anisotropic Gaussian kernels are sufficient for image SR. To address this issue, this paper proposes a normalizing flow-based kernel prior (FKP) for kernel modeling. By learning an invertible mapping be- tween the anisotropic Gaussian kernel distribution and a tractable latent distribution, FKP can be easily used to re- place the kernel modeling modules of Double-DIP and Ker- nelGAN. Specifically, FKP optimizes the kernel in the la- tent space rather than the network parameter space, which allows it to generate reasonable kernel initialization, tra- verse the learned kernel manifold and improve the optimiza- tion stability. Extensive experiments on synthetic and real- world images demonstrate that the proposed FKP can sig- nificantly improve the kernel estimation accuracy with less parameters, runtime and memory usage, leading to state- of-the-art blind SR results.
- We propose a kernel prior named FKP that is applica- ble for arbitrary blur kernel modeling. It learns a bi- jective mapping between the kernel and the latent vari- able. To the best of our knowledge, FKP is the first learning-based kernel prior.
- Byfixingitsparametersandoptimizingthelatentvari-able, FKP traverses the learned kernel manifold and searches for the kernel prediction, ensuring reasonable kernels for initialization and along optimization.
- With less parameters, runtime and memory usage, FKP improves the stability and accuracy of existing kernel estimation methods including Double-DIP and Ker- nelGAN, leading to state-of-the-art blind SR perfor- mance.
### 3.Kim, Soo Ye, Hyeonjun Sim, and Munchurl Kim. "KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:
Blind super-resolution (SR) methods aim to generate a high quality high resolution image from a low resolution image containing unknown degradations. However, natural images contain various types and amounts of blur: some may be due to the inherent degradation characteristics of the camera, but some may even be intentional, for aesthetic purposes (e.g. Bokeh effect). In the case of the latter, it becomes highly difficult for SR methods to disentangle the blur to remove, and that to leave as is. In this paper, we pro- pose a novel blind SR framework based on kernel-oriented adaptive local adjustment (KOALA) of SR features, called KOALAnet, which jointly learns spatially-variant degrada- tion and restoration kernels in order to adapt to the spatially- variant blur characteristics in real images. Our KOALAnet outperforms recent blind SR methods for synthesized LR im- ages obtained with randomized degradations, and we further show that the proposed KOALAnet produces the most natural results for artistic photographs with intentional blur, which are not over-sharpened, by effectively handling images mixed with in-focus and out-of-focus areas.
- We propose a blind SR framework that jointly learns spatially-variant degradation and restoration kernels. The restoration (upsampling) network leverages novel
- KOALA modules to adaptively adjust the SR fea- tures based on the predicted degradation kernels. The KOALA modules are extensible, and can be inserted into any CNN architecture for image restoration tasks.
- We empirically show that the proposed KOALAnet out- performs the recent state-of-the-art blind SR methods for synthesized LR images obtained under randomized degradation conditions, as well as for historic LR im- ages with unknown degradations.
- We first analyze SR results on images mixed with in-focus and out-of-focus regions, showing that our KOALAnet is able to discern intentionally blurry areas and process them accordingly, leaving the photogra- pher’s intent unchanged after SR.
### 4.Wang, Longguang, et al. "Exploring Sparsity in Image Super-Resolution for Efficient Inference." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:Current CNN-based super-resolution (SR) methods pro- cess all locations equally with computational resources be- ing uniformly assigned in space. However, since missing details in low-resolution (LR) images mainly exist in re- gions of edges and textures, less computational resources are required for those flat regions. Therefore, existing CNN- based methods involve redundant computation in flat re- gions, which increases their computational cost and lim- its their applications on mobile devices. In this paper, we explore the sparsity in image SR to improve inference effi- ciency of SR networks. Specifically, we develop a Sparse Mask SR (SMSR) network to learn sparse masks to prune redundant computation. Within our SMSR, spatial masks learn to identify “important” regions while channel masks learn to mark redundant channels in those “unimportant” regions. Consequently, redundant computation can be ac- curately localized and skipped while maintaining compa- rable performance. It is demonstrated that our SMSR achieves state-of-the-art performance with 41%/33%/27% FLOPs being reduced for ×2/3/4 SR. Code is available at: https://github.com/LongguangWang/SMSR
- 1) We develop an SMSR network to dynamically skip redundant computation for efficient image SR. In contrast to existing works that focus on lightweight network designs, we ex- plore a different route by pruning redundant computation to improve inference efficiency. 2) We propose to local- ize redundant computation by learning spatial and channel masks. These two kinds of masks work jointly for fine- grained localization of redundant computation. 3) Experi- mental results show that our SMSR achieves state-of-the-art performance with better inference efficiency. For example, our SMSR outperforms previous methods on Set14 for ×2 SR with a significant speedup on mobile devices (Table 2).
### 5.Kong, Xiangtao, et al. "ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually de- composed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline – ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub- images into different classes according to restoration diffi- culties, then applies an SR-Module to perform SR for differ- ent classes. The Class-Module is a conventional classifica- tion network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classifica- tion method with two losses – Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks.
- We pro- pose ClassSR. It is the first SR pipeline that incorpo- rates classification and super-resolution together on the sub- image level.
- We tackle acceleration by the character- istic of data. It makes ClassSR orthogonal to other accel- eration networks. A network compressed to the limit can still be accelerated by ClassSR.
- We propose a clas- sification method with two novel losses. It divides sub- images according to their restoration difficulties that are processed by a specific branch instead of predetermined la- bels, so it can also be directly applied to other low-level vision tasks. 
### 6.Mei, Yiqun, Yuchen Fan, and Yuqian Zhou. "Image Super-Resolution With Non-Local Sparse Attention." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:Both Non-Local (NL) operation and sparse representa- tion are crucial for Single Image Super-Resolution (SISR). In this paper, we investigate their combinations and propose a novel Non-Local Sparse Attention (NLSA) with dynamic sparse attention pattern. NLSA is designed to retain long- range modeling capability from NL operation while enjoy- ing robustness and high-efficiency of sparse representation. Specifically, NLSA rectifies non-local attention with spher- ical locality sensitive hashing (LSH) that partitions the in- put space into hash buckets of related features. For every query signal, NLSA assigns a bucket to it and only com- putes attention within the bucket. The resulting sparse at- tention prevents the model from attending to locations that are noisy and less-informative, while reducing the computa- tional cost from quadratic to asymptotic linear with respect to the spatial size. Extensive experiments validate the effec- tiveness and efficiency of NLSA. With a few non-local sparse attention modules, our architecture, called non-local sparse network (NLSN), reaches state-of-the-art performance for SISR quantitatively and qualitatively. 
- We propose to enforce sparsity in Non-Local opera- tion for SISR task via a novel Non-Local Sparse At- tention (NLSA) module. The sparsity constraint forces the module to focus on correlated and informative area while ignoring unrelated and noisy contents.
- We achieve the feature sparsity by first grouping the feature pixels and only conducting Non-Local oper- ations within the group named attention bucket. We adopt the Locality Sensitive Hashing (LSH) for group- ing and assign each group a Hash code. The proposed approach significantly reduces the computational com- plexity from quadratic to asymptotic linear.
- Without any bells and whistles, a few NLSA modules can drive a fairly simple ResNet backbone to state-of- the-arts. Extensive experiments demonstrate the ad- vantages of NLSA over the standard Non-Local Atten- tion (NLA).
### 7.Wei, Yunxuan, et al. "Unsupervised real-world image super resolution via domain-distance aware training." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:These days, unsupervised super-resolution (SR) is soar- ing due to its practical and promising potential in real sce- narios. The philosophy of off-the-shelf approaches lies in the augmentation of unpaired data, i.e. first generating syn- thetic low-resolution (LR) images Yg corresponding to real- world high-resolution (HR) images X r in the real-world LR domain Yr, and then utilizing the pseudo pairs {Yg,Xr} for training in a supervised manner. Unfortunately, since image translation itself is an extremely challenging task, the SR performance of these approaches is severely lim- ited by the domain gap between generated synthetic LR images and real LR images. In this paper, we propose a novel domain-distance aware super-resolution (DASR) ap- proach for unsupervised real-world image SR. The domain gap between training data (e.g. Yg) and testing data (e.g. Yr) is addressed with our domain-gap aware training and domain-distance weighted supervision strategies. Domain- gap aware training takes additional benefit from real data in the target domain while domain-distance weighted su- pervision brings forward the more rational use of labeled source domain data. The proposed method is validated on synthetic and real datasets and the experimental results show that DASR consistently outperforms state-of-the-art unsupervised SR approaches in generating SR outputs with more realistic and natural textures. 
- A domain distance aware super-resolution (DASR) framework is proposed to solve the real-world image SR problem. DASR addresses the domain gap between generated LR images and real images with the pro- posed domain-gap aware training and domain-distance weighted supervision strategies.
- We provide detailed ablation studies to analyze and validate our contributions. Experimental results on synthetic and real datasets clearly demonstrate the su- periority of DASR over the competing approaches.
### 8.Wang, Longguang, et al. "Unsupervised Degradation Representation Learning for Blind Super-Resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:
Most existing CNN-based super-resolution (SR) methods are developed based on an assumption that the degradation is fixed and known (e.g., bicubic downsampling). However, these methods suffer a severe performance drop when the real degradation is different from their assumption. To han- dle various unknown degradations in real-world applica- tions, previous methods rely on degradation estimation to reconstruct the SR image. Nevertheless, degradation esti- mation methods are usually time-consuming and may lead to SR failure due to large estimation errors. In this pa- per, we propose an unsupervised degradation representa- tion learning scheme for blind SR without explicit degrada- tion estimation. Specifically, we learn abstract representa- tions to distinguish various degradations in the representa- tion space rather than explicit estimation in the pixel space. Moreover, we introduce a Degradation-Aware SR (DASR) network with flexible adaption to various degradations based on the learned representations. It is demonstrated that our degradation representation learning scheme can extract discriminative representations to obtain accurate degradation information. Experiments on both synthetic and real images show that our network achieves state-of- the-art performance for the blind SR task. Code is avail- able at: https://github.com/LongguangWang/ DASR.
### 9.Bhat, Goutam, et al. "Deep burst super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:While single-image super-resolution (SISR) has at- tracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame super- resolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increas- ing popularity of burst photography, have made MFSR an important problem for real-world applications.
We propose a novel architecture for the burst super- resolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise opti- cal flow. The information from all frames are then adap- tively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth. We perform comprehensive experimental analysis, demon- strating the effectiveness of the proposed architecture.
- (i) We introduce the first real world burst super-resolution dataset consisting of RAW bursts and cor- responding HR ground truths. (ii) We propose a novel MFSR architecture which can perform joint denoising, de- mosaicking, and SR using bursts captured from a handheld camera. (iii) Our architecture employs an attention-based fusion method to adaptively merge the input images to gen- erate high quality HR output (iv) We further address mis-alignment issues encountered when training on real world data by introducing a loss function which can internally cor- rect these mis-alignments.
### 10.Lu, Liying, et al. "MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:Reference-based image super-resolution (RefSR) has shown promising success in recovering high-frequency de- tails by utilizing an external reference image (Ref). In this task, texture details are transferred from the Ref image to the low-resolution (LR) image according to their point- or patch-wise correspondence. Therefore, high-quality cor- respondence matching is critical. It is also desired to be computationally efficient. Besides, existing RefSR methods tend to ignore the potential large disparity in distributions between the LR and Ref images, which hurts the effective- ness of the information utilization. In this paper, we pro- pose the MASA network for RefSR, where two novel mod- ules are designed to address these problems. The proposed Match & Extraction Module significantly reduces the com- putational cost by a coarse-to-fine correspondence match- ing scheme. The Spatial Adaptation Module learns the dif- ference of distribution between the LR and Ref images, and remaps the distribution of Ref features to that of LR fea- tures in a spatially adaptive way. This scheme makes the network robust to handle different reference images. Exten- sive quantitative and qualitative experiments validate the effectiveness of our proposed model.
- The proposed Match & Extraction Module signifi- cantly reduces the computational cost of correspon- dence matching in the deep feature space. Our results show that a two-orders-of-magnitude reduction mea- sured in FLOPS is achieved.
- The proposed Spatial Adaptation Module is robust to Ref images with different color and luminance distri- butions. It enables the network to better utilize useful information extracted from Ref images.
### 11.Zhou, Yuemei, et al. "Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:Various combinations of cameras enrich computa- tional photography, among which reference-based super- resolution (RefSR) plays a critical role in multiscale imag- ing systems. However, existing RefSR approaches fail to accomplish high-fidelity super-resolution under a large res- olution gap, e.g., 8× upscaling, due to the lower consider- ation of the underlying scene structure. In this paper, we aim to solve the RefSR problem in actual multiscale camera systems inspired by multiplane image (MPI) representation. Specifically, we propose Cross-MPI, an end-to-end RefSR network composed of a novel plane-aware attention-based MPI mechanism, a multiscale guided upsampling module as well as a super-resolution (SR) synthesis and fusion module. Instead of using a direct and exhaustive matching between the cross-scale stereo, the proposed plane-aware attention mechanism fully utilizes the concealed scene structure for efficient attention-based correspondence searching. Fur- ther combined with a gentle coarse-to-fine guided upsam- pling strategy, the proposed Cross-MPI can achieve a ro- bust and accurate detail transmission. Experimental results on both digitally synthesized and optical zoom cross-scale data show that the Cross-MPI framework can achieve su- perior performance against the existing RefSR methods and is a real fit for actual multiscale camera systems even with large-scale differences.
- We take a close look at the RefSR problem through the lens of the MPI representation, achieving super- resolution up to 8× on real hybrid imaging systems.
- We propose a novel plane-aware attention mechanism for MPI estimation that can achieve more explicit and efficient correspondence estimation compared with the original direct concatenation-and-convolution opera- tion.
- We propose a novel multiscale guided upsampling module for cross-scale multiplane image synthesis that can solve the matching problem under large resolution differences. A fine-detailed depth map that encodes the scene structure can also be inferred.
### 12.Jo, Younghyun, et al. "Tackling the Ill-Posedness of Super-Resolution Through Adaptive Target Generation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
- abstract:By the one-to-many nature of the super-resolution (SR) problem, a single low-resolution (LR) image can be mapped to many high-resolution (HR) images. However, learning based SR algorithms are trained to map an LR image to the corresponding ground truth (GT) HR image in the training dataset. The training loss will increase and penalize the algorithm when the output does not exactly match the GT target, even when the outputs are mathematically valid can- didates according to the SR framework. This becomes more problematic for the blind SR, as diverse unknown blur ker- nels exacerbate the ill-posedness of the problem. To this end, we propose a fundamentally different approach for the SR by introducing the concept of the adaptive target. The adaptive target is generated from the original GT target by a transformation to match the output of the SR network. The adaptive target provides an effective way for the SR algo- rithm to deal with the ill-posed nature of the SR, by provid- ing the algorithm with the flexibility of accepting a variety of valid solutions. Experimental results show the effectiveness of our algorithm, especially for improving the perceptual quality of HR outputs.
- We introduce a simple and effective way to encourage sharp output generation using proposed adaptive target as a solution for the one-to-many problem of SR. For blind SR, our method is fundamentally different from previous works as our algorithm works in a single-shot manner without the blur kernel estimation.
- The adaptive target is created on-the-fly during the train- ing stage with little computational overhead, therefore, it is applicable to any training dataset without preprocess- ing. In addition, our framework can be attached to any deep SR models as a loss function.
- Our method outperforms previous state-of-the-art blind SR methods in terms of PSNR and visual quality. In non- blind scenario, our method generates consistent details when combined with GANs for perceptual SR, achieving both higher PSNR and LPIPS metrics at the same time.
## CVPR 2020
### 1.Zhang, Kai, Luc Van Gool, and Radu Timofte. "Deep unfolding network for image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- abstract:Learning-based single image super-resolution (SISR) methods are continuously showing superior effective- ness and efficiency over traditional model-based methods, largely due to the end-to-end training. However, different from model-based methods that can handle the SISR prob- lem with different scale factors, blur kernels and noise lev- els under a unified MAP (maximum a posteriori) frame- work, learning-based methods generally lack such flexibil- ity. To address this issue, this paper proposes an end-to-end trainable unfolding network which leverages both learning- based methods and model-based methods. Specifically, by unfolding the MAP inference via a half-quadratic splitting algorithm, a fixed number of iterations consisting of alter- nately solving a data subproblem and a prior subproblem can be obtained. The two subproblems then can be solved with neural modules, resulting in an end-to-end trainable, iterative network. As a result, the proposed network inher- its the flexibility of model-based methods to super-resolve blurry, noisy images for different scale factors via a single model, while maintaining the advantages of learning-based methods. Extensive experiments demonstrate the superior- ity of the proposed deep unfolding network in terms of flex- ibility, effectiveness and also generalizability. 
- An end-to-end trainable unfolding super-resolution network (USRNet) is proposed. USRNet is the first attempt to handle the classical degradation model with different scale factors, blur kernels and noise levels via a single end-to-end trained model.
- USRNet integrates the flexibility of model-based methods and the advantages of learning-based meth- ods, providing an avenue to bridge the gap between model-based and learning-based methods.
- USRNet intrinsically imposes a degradation constraint (i.e., the estimated HR image should accord with the degradation process) and a prior constraint (i.e., the es- timated HR image should have natural characteristics) on the solution.
- USRNet performs favorably on LR images with dif- ferent degradation settings, showing great potential for practical applications.
### 2.Ma, Cheng, et al. "Structure-preserving super resolution with gradient guidance." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- abstract:Structures matter in single image super resolution (SISR). Recent studies benefiting from generative adversar- ial network (GAN) have promoted the development of SISR by recovering photo-realistic images. However, there are always undesired structural distortions in the recovered im- ages. In this paper, we propose a structure-preserving su- per resolution method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Specifically, we exploit gradi- ent maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure pri- ors for the SR process. On the other hand, we propose a gradient loss which imposes a second-order restriction on the super-resolved images. Along with the previous image- space loss functions, the gradient-space objectives help generative networks concentrate more on geometric struc- tures. Moreover, our method is model-agnostic, which can be potentially used for off-the-shelf SR networks. Experi- mental results show that we achieve the best PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with state-of-the-art perceptual-driven SR meth- ods. Visual results demonstrate our superiority in restoring structures while generating natural SR images.
### 3.Maeda, Shunta. "Unpaired image super-resolution using pseudo-supervision." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- abstract:In most studies on learning-based image super- resolution (SR), the paired training dataset is created by downscaling high-resolution (HR) images with a predeter- mined operation (e.g., bicubic). However, these methods fail to super-resolve real-world low-resolution (LR) images, for which the degradation process is much more compli- cated and unknown. In this paper, we propose an unpaired SR method using a generative adversarial network that does not require a paired/aligned training dataset. Our network consists of an unpaired kernel/noise correction network and a pseudo-paired SR network. The correction network re- moves noise and adjusts the kernel of the inputted LR im- age; then, the corrected clean LR image is upscaled by the SR network. In the training phase, the correction network also produces a pseudo-clean LR image from the inputted HR image, and then a mapping from the pseudo-clean LR image to the inputted HR image is learned by the SR net- work in a paired manner. Because our SR network is in- dependent of the correction network, well-studied existing network architectures and pixel-wise loss functions can be integrated with the proposed framework. Experiments on diverse datasets show that the proposed method is superior to existing solutions to the unpaired SR problem.
### 4.Liu, Jie, et al. "Residual feature aggregation network for image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- abstract:Recently, very deep convolutional neural networks (CNNs) have shown great power in single image super- resolution (SISR) and achieved significant improvements against traditional methods. Among these CNN-based methods, the residual connections play a critical role in boosting the network performance. As the network depth grows, the residual features gradually focused on different aspects of the input image, which is very useful for recon- structing the spatial details. However, existing methods ne- glect to fully utilize the hierarchical features on the resid- ual branches. To address this issue, we propose a novel residual feature aggregation (RFA) framework for more effi- cient feature extraction. The RFA framework groups several residual modules together and directly forwards the fea- tures on each local residual branch by adding skip connec- tions. Therefore, the RFA framework is capable of aggre- gating these informative residual features to produce more representative features. To maximize the power of the RFA framework, we further propose an enhanced spatial atten- tion (ESA) block to make the residual features to be more focused on critical spatial contents. The ESA block is de- signed to be lightweight and efficient. Our final RFANet is constructed by applying the proposed RFA framework with the ESA blocks. Comprehensive experiments demonstrate the necessity of our RFA framework and the superiority of our RFANet over state-of-the-art SISR methods.
- We propose a general residual feature aggregation (RFA) framework for more accurate image SR. Com- prehensive ablation study shows that the performance of residual networks as well as dense networks can get a substantial improvement.
- We propose an enhanced spatial attention (ESA) block to adaptively rescale features according to the spatial context. The ESA block allows the network to learn more discriminative features. Besides, it is lightweight and has better performance than the plain spatial atten- tion block.
- We propose a residual feature aggregation network (RFANet) which is constructed by incorporating the proposed RFA framework with the powerful ESA block. Thanks to the enhanced spatial attention mech- anism, the RFA framework can aggregate more repre- sentative features, thus generating more accurate SR results.
### 5.Guo, Yong, et al. "Closed-loop matters: Dual regression networks for single image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- Deep neural networks have exhibited promising perfor- mance in image super-resolution (SR) by learning a non- linear mapping function from low-resolution (LR) images to high-resolution (HR) images. However, there are two un- derlying limitations to existing SR methods. First, learning the mapping function from LR to HR images is typically an ill-posed problem, because there exist infinite HR images that can be downsampled to the same LR image. As a result, the space of the possible functions can be extremely large, which makes it hard to find a good solution. Second, the paired LR-HR data may be unavailable in real-world ap- plications and the underlying degradation method is often unknown. For such a more general case, existing SR mod- els often incur the adaptation problem and yield poor per- formance. To address the above issues, we propose a dual regression scheme by introducing an additional constraint on LR data to reduce the space of the possible functions. Specifically, besides the mapping from LR to HR images, we learn an additional dual regression mapping estimates the down-sampling kernel and reconstruct LR images, which forms a closed-loop to provide additional supervision. More critically, since the dual regression process does not depend on HR images, we can directly learn from LR images. In this sense, we can easily adapt SR models to real-world data, e.g., raw video frames from YouTube. Extensive exper- iments with paired training data and unpaired real-world data demonstrate our superiority over existing methods.
- We develop a dual regression scheme by introduc- ing an additional constraint such that the mappings can form a closed-loop and LR images can be recon- structed to enhance the performance of SR models. Moreover, we also theoretically analyze the general- ization ability of the proposed scheme, which further confirms its superiority to existing methods.
- We study a more general super-resolution case where there is no corresponding HR data w.r.t. the real-world LR data. With the proposed dual regression scheme, deep models can be easily adapted to real-world data, e.g., raw video frames from YouTube.
- Extensive experiments on both the SR tasks with paired training data and unpaired real-world data demonstrate the effectiveness of the proposed dual re- gression scheme in image super-resolution
### 6.Mei, Yiqun, et al. "Image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range feature- wise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correla- tion by exploring non-local attention modules. However, none of the current deep models have studied another in- herent property of images: cross-scale feature correlation. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correla- tions within a single low-resolution (LR) image. The perfor- mance of SISR is significantly improved by exhaustively in- tegrating all possible priors. Extensive experiments demon- strate the effectiveness of the proposed CS-NL module by setting new state-of-the-arts on multiple SISR benchmarks.
- The core contribution of the paper is to propose the first Cross-Scale Non-Local (CS-NL) attention mod- ule in deep networks for SISR task. We explicitly formulate the pixel-to-patch and patch-to-patch simi- larities inside the image, and demonstrate that addi- tionally mining cross-scale self-similarities greatly im- proves the SISR performance.
- We then propose a powerful Self-Exemplar Mining (SEM) cell to fuse information recurrently. Inside the cell, we exhaustively mine all the possible intrinsic pri- ors by combining local, in-scale non-local, and the pro- posed cross-scale non-local feature correlations, and embrace rich external statistics learned by the network.
- The newly proposed recurrent SR network achieves the state-of-the-art performance on multiple image bench- marks. Extensive ablation experiments further verify the effectiveness of the novel network.
### 7.Yang, Fuzhi, et al. "Learning texture transformer network for image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
- We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR ap- proaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which lim- its these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Im- age Super-Resolution (TTSR), in which the LR and Ref im- ages are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a soft- attention module for texture synthesis. Such a design en- courages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be trans- ferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recov- ery from different levels (e.g., from 1× to 4× magnifica- tion). Extensive experiments show that TTSR achieves sig- nificant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.
- To the best of our knowledge, we are one of the first to introduce the transformer architecture into image gen- eration tasks. More specifically, we propose a texture transformer with four closely-related modules for im- age SR which achieves significant improvements over SOTA approaches.
- We propose a novel cross-scale feature integration module for image generation tasks which enables our approach to learn a more powerful feature representa- tion by stacking multiple texture transformers.
## ECCV 2020
### 1.Lugmayr, Andreas, et al. "Srflow: Learning the super-resolution space with normalizing flow." European Conference on Computer Vision. Springer, Cham, 2020.
-  Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combina- tions of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method ca- pable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner us- ing a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we uti- lize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive ex- periments on faces, as well as on super-resolution in general. SRFlow out- performs state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions. Code and trained models will be available at: git.io/SRFlow
-  (i) We are the first to design a conditional normalizing flow archi- tecture that achieves state-of-the-art super-resolution quality. (ii) We harness the strong HR distribution learned by SRFlow to develop novel techniques for controlled image manipulation and editing. (iii) Although only trained for super- resolution, we show that SRFlow is capable of image denoising and restoration. (iv) Comprehensive experiments for face and general image super-resolution show that our approach outperforms state-of-the-art GAN-based methods for both perceptual and reconstruction-based metrics.
### 2.Luo, Xiaotong, et al. "Latticenet: Towards lightweight image super-resolution with lattice block." Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII 16. Springer International Publishing, 2020.
- Deep neural networks with a massive number of layers have made a remarkable breakthrough on single image super-resolution (SR), but sacrifice computation complexity and memory storage. To address this problem, we focus on the lightweight models for fast and accurate image SR. Due to the frequent use of residual block (RB) in SR models, we pursue an economical structure to adaptively combine RBs. Draw- ing lessons from lattice filter bank, we design the lattice block (LB) in which two butterfly structures are applied to combine two RBs. LB has the potential of various linear combinations of two RBs. Each case of LB depends on the combination coefficients which are determined by the attention mechanism. LB favors the lightweight SR model with the reduction of about half amount of the parameters while keeping the similar SR performance. Moreover, we propose a lightweight SR model, LatticeNet, which uses series connection of LBs and the backward fea- ture fusion. Extensive experiments demonstrate that our proposal can achieve superior accuracy on four available benchmark datasets against other state-of-the-art methods, while maintaining relatively low compu- tation and memory requirements.
### 3.Niu, Ben, et al. "Single image super-resolution via a holistic attention network." European Conference on Computer Vision. Springer, Cham, 2020.
- Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer. However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hi- erarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selec- tively capture more informative features. Extensive experiments demon- strate that the proposed HAN performs favorably against the state-of- the-art single image super-resolution approaches.
- We propose a novel super-resolution algorithm named Holistic Attention Net- work (HAN), which enhances the representational ability of feature represen- tations for super-resolution.
- We introduce a layer attention module (LAM) to learn the weights for hier- archical features by considering correlations of multi-scale layers. Meanwhile, a channel-spatial attention module (CSAM) is presented to learn the channel and spatial interdependencies of features in each layer.
- The proposed two attention modules collaboratively improve the SR results by modeling informative features among hierarchical layers, channels, and po- sitions. Extensive experiments demonstrate that our algorithm performs fa- vorably against the state-of-the-art SISR approaches.
## ICCV 2019
### 1.Zhou, Ruofan, and Sabine Susstrunk. "Kernel modeling super-resolution on real low-resolution images." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
- Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super- resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their train- ing data: low-resolution (LR) images are obtained by bicu- bic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images.
To improve generalization and robustness of deep super- resolution CNNs on real photographs, we present a ker- nel modeling super-resolution network (KMSR) that incor- porates blur-kernel modeling in the training. Our pro- posed KMSR consists of two stages: we first build a pool of realistic blur-kernels with a generative adversarial net- work (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental vali- dations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels.
### 2.Rad, Mohammad Saeed, et al. "Srobb: Targeted perceptual loss for single image super-resolution." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
- By benefiting from perceptual losses, recent studies have improved significantly the performance of the super- resolution task, where a high-resolution image is resolved from its low-resolution counterpart. Although such objec- tive functions generate near-photorealistic results, their ca- pability is limited, since they estimate the reconstruction error for an entire image in the same way, without con- sidering any semantic information. In this paper, we pro- pose a novel method to benefit from perceptual loss in a more objective way. We optimize a deep network-based de- coder with a targeted objective function that penalizes im- ages at different semantic levels using the corresponding terms. In particular, the proposed method leverages our proposed OBB (Object, Background and Boundary) labels, generated from segmentation labels, to estimate a suitable perceptual loss for boundaries, while considering texture similarity for backgrounds. We show that our proposed ap- proach results in more realistic textures and sharper edges, and outperforms other state-of-the-art algorithms in terms of both qualitative results on standard benchmarks and re- sults of extensive user studies.
### 3.Cai, Jianrui, et al. "Toward real-world single image super-resolution: A new benchmark and a new model." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
- Most of the existing learning-based single image super- resolution (SISR) methods are trained and evaluated on simulated datasets, where the low-resolution (LR) images are generated by applying a simple and uniform degrada- tion (i.e., bicubic downsampling) to their high-resolution (HR) counterparts. However, the degradations in real- world LR images are far more complicated. As a con- sequence, the SISR models trained on simulated data be- come less effective when applied to practical scenarios. In this paper, we build a real-world super-resolution (Re- alSR) dataset where paired LR-HR images on the same scene are captured by adjusting the focal length of a dig- ital camera. An image registration algorithm is developed to progressively align the image pairs at different resolu- tions. Considering that the degradation kernels are natu- rally non-uniform in our dataset, we present a Laplacian pyramid based kernel prediction network (LP-KPN), which efficiently learns per-pixel kernels to recover the HR image. Our extensive experiments demonstrate that SISR models trained on our RealSR dataset deliver better visual quality with sharper edges and finer textures on real-world scenes than those trained on simulated datasets. Though our Re- alSR dataset is built by using only two cameras (Canon 5D3 and Nikon D810), the trained model generalizes well to other camera devices such as Sony a7II and mobile phones.
- We build a RealSR dataset consisting of precisely aligned HR and LR image pairs with different scal- ing factors, providing a general purpose benchmark for real-world SISR model training and evaluation.
- We present an LP-KPN model and validate its effi- ciency and effectiveness in real-world SISR.
### 4.Zhang, Wenlong, et al. "Ranksrgan: Generative adversarial networks with ranker for image super-resolution." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
- Generative Adversarial Networks (GAN) have demon- strated the potential to recover realistic details for single image super-resolution (SISR). To further improve the vi- sual quality of super-resolved results, PIRM2018-SR Chal- lenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing meth- ods cannot directly optimize these indifferentiable percep- tual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super- Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of per- ceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then in- troduce a novel rank-content loss to optimize the percep- tual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/ Projects/RankSRGAN
### 5.Shaham, Tamar Rott, Tali Dekel, and Tomer Michaeli. "Singan: Learning a generative model from a single natural image." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
- We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolu- tional GANs, each responsible for learning the patch distri- bution at a different scale of the image. This allows generat- ing new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global struc- ture and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it gen- erates samples from noise). User studies confirm that the generated samples are commonly confused to be real im- ages. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.
## CVPR 2019
### 1.Wang, Xintao, et al. "Deep network interpolation for continuous imagery effect transition." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Deep convolutional neural network has demonstrated its capability of learning a deterministic mapping for the de- sired imagery effect. However, the large variety of user flavors motivates the possibility of continuous transition among different output effects. Unlike existing methods that require a specific design to achieve one particular transition (e.g., style transfer), we propose a simple yet universal ap- proach to attain a smooth control of diverse imagery effects in many low-level vision tasks, including image restoration, image-to-image translation, and style transfer. Specifically, our method, namely Deep Network Interpolation (DNI), ap- plies linear interpolation in the parameter space of two or more correlated networks. A smooth control of imagery effects can be achieved by tweaking the interpolation co- efficients. In addition to DNI and its broad applications, we also investigate the mechanism of network interpolation from the perspective of learned filters.
### 2.Zhang, Kai, Wangmeng Zuo, and Lei Zhang. "Deep plug-and-play super-resolution for arbitrary blur kernels." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- While deep neural networks (DNN) based single image super-resolution (SISR) methods are rapidly gaining popu- larity, they are mainly designed for the widely-used bicubic degradation, and there still remains the fundamental chal- lenge for them to super-resolve low-resolution (LR) image with arbitrary blur kernels. In the meanwhile, plug-and- play image restoration has been recognized with high flex- ibility due to its modular structure for easy plug-in of de- noiser priors. In this paper, we propose a principled for- mulation and framework by extending bicubic degradation based deep SISR with the help of plug-and-play framework to handle LR images with arbitrary blur kernels. Specifical- ly, we design a new SISR degradation model so as to take advantage of existing blind deblurring methods for blur k- ernel estimation. To optimize the new degradation induced energy function, we then derive a plug-and-play algorith- m via variable splitting technique, which allows us to plug any super-resolver prior rather than the denoiser prior as a modular part. Quantitative and qualitative evaluations on synthetic and real LR images demonstrate that the proposed deep plug-and-play super-resolution framework is flexible and effective to deal with blurry LR images.
- Amorerealisticdegradationmodelthanbicubicdegra- dation model for SISR is proposed. It considers arbi- trary blur kernels and enables to use existing deblur- ring methods for blur kernel estimation.
- A deep plug-and-play super-resolution framework is proposed to solve SISR with the new degradation mod- el. DPSR is applicable beyond bicubic degradation and can handle LR images with arbitrary blur kernels.
- The proposed DPSR is well-principled as the iterative scheme aims to solve the new degradation induced en- ergy function.
- The proposed DPSR extends existing plug-and-play framework, showing that the plug-and-play prior for SISR is not limited to Gaussian denoiser.
### 3.Li, Zhen, et al. "Feedback network for image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Recent advances in image super-resolution (SR) ex- plored the power of deep learning to achieve a better re- construction performance. However, the feedback mecha- nism, which commonly exists in human visual system, has not been fully exploited in existing deep learning based image SR methods. In this paper, we propose an image super-resolution feedback network (SRFBN) to refine low- level representations with high-level information. Specifi- cally, we use hidden states in a recurrent neural network (RNN) with constraints to achieve such feedback manner. A feedback block is designed to handle the feedback con- nections and to generate powerful high-level representa- tions. The proposed SRFBN comes with a strong early re- construction ability and can create the final high-resolution image step by step. In addition, we introduce a curricu- lum learning strategy to make the network well suitable for more complicated tasks, where the low-resolution im- ages are corrupted by multiple types of degradation. Ex- tensive experimental results demonstrate the superiority of the proposed SRFBN in comparison with the state-of-the- art methods. 
### 4.Zhang, Zhifei, et al. "Image super-resolution by neural texture transfer." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Due to the significant information loss in low-resolution (LR) images, it has become extremely challenging to further advance the state-of-the-art of single image super-resolu- tion (SISR). Reference-based super-resolution (RefSR), on the other hand, has proven to be promising in recovering high-resolution (HR) details when a reference (Ref) image with similar content as that of the LR input is given. How- ever, the quality of RefSR can degrade severely when Ref is less similar. This paper aims to unleash the potential of RefSR by leveraging more texture details from Ref images with stronger robustness even when irrelevant Ref images are provided. Inspired by the recent work on image styl- ization, we formulate the RefSR problem as neural texture transfer. We design an end-to-end deep model which en- riches HR details by adaptively transferring the texture from Ref images according to their textural similarity. Instead of matching content in the raw pixel space as done by previous methods, our key contribution is a multi-level matching con- ducted in the neural space. This matching scheme facilitates multi-scale neural transfer that allows the model to bene- fit more from those semantically related Ref patches, and gracefully degrade to SISR performance on the least rele- vant Ref inputs. We build a benchmark dataset for the gen- eral research of RefSR, which contains Ref images paired with LR inputs with varying levels of similarity. Both quan- titative and qualitative evaluations demonstrate the superi- ority of our method over state-of-the-art1.
- We explore a more general RefSR problem, breaking the performance barrier in SISR (i.e., lack of texture detail) and relaxing constraints in existing RefSR (i.e., alignment assumption).
- We propose an end-to-end deep model, SRNTT, for the RefSR problem to recover the LR image condi- tioned on any given references by multi-scale neural texture transfer. We demonstrate the visual improve- ment, effectiveness, and adaptiveness of the proposed SRNTT by extensive empirical studies.
- We build a benchmark dataset, CUFED5, to facili- tate the further research and performance evaluation of RefSR methods in handling references with different levels of similarity to the LR input image.
### 5.Dai, Tao, et al. "Second-order attention network for single image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to ex- plore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order atten- tion network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel train- able second-order channel attention (SOCA) module is de- veloped to adaptively rescale the channel-wise features by using second-order feature statistics for more discrimina- tive representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn in- creasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality.
- We propose a deep second-order attention network (SAN) for accurate image SR. Extensive experiments on public datasets demonstrate the superiority of our SAN over state-of-the-art methods in terms of both quantitive and visual quality.
- We propose second-order channel attention (SOCA) mechanism to adaptively rescale features by consid- ering feature statistics higher than first-order. Such SOCA mechanism allows our network to focus on more informative features and enhance discriminative learning ability. Besides, we also utilize an iterative method for covariance normalization to speed up the training of our network.
- We propose non-locally enhanced residual group (NLRG) structure to build a deep network, which fur- ther incorporates non-local operations to capture spa- tial contextual information, and share-source residual group structure to learn deep features. Besides, the share-source residual group structure through share- source skip connections could allow more abundant in- formation from the LR input to be bypassed and ease the training of the deep network.
### 6.Xu, Xiangyu, Yongrui Ma, and Wenxiu Sun. "Towards real scene super-resolution with raw images." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Most existing super-resolution methods do not perform well in real scenarios due to lack of realistic training data and information loss of the model input. To solve the first problem, we propose a new pipeline to generate realistic training data by simulating the imaging process of digital cameras. And to remedy the information loss of the input, we develop a dual convolutional neural network to exploit the originally captured radiance information in raw images. In addition, we propose to learn a spatially-variant color transformation which helps more effective color correction- s. Extensive experiments demonstrate that super-resolution with raw data helps recover fine details and clear struc- tures, and more importantly, the proposed network and data generation pipeline achieve superior results for single im- age super-resolution in real scenarios. 
### 7.Gu, Jinjin, et al. "Blind super-resolution with iterative kernel correction." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Deep learning based methods have dominated super- resolution (SR) field due to their remarkable performance in terms of effectiveness and efficiency. Most of these methods assume that the blur kernel during downsampling is prede- fined/known (e.g., bicubic). However, the blur kernels in- volved in real applications are complicated and unknown, resulting in severe performance drop for the advanced SR methods. In this paper, we propose an Iterative Kernel Cor- rection (IKC) method for blur kernel estimation in blind SR problem, where the blur kernels are unknown. We draw the observation that kernel mismatch could bring regular ar- tifacts (either over-sharpening or over-smoothing), which can be applied to correct inaccurate blur kernels. Thus we introduce an iterative correction scheme – IKC that achieves better results than direct kernel estimation. We fur- ther propose an effective SR network architecture using spa- tial feature transform (SFT) layers to handle multiple blur kernels, named SFTMD. Extensive experiments on synthetic and real-world images show that the proposed IKC method with SFTMD can provide visually favorable SR results and the state-of-the-art performance in blind SR problem.
### 8.He, Xiangyu, et al. "Ode-inspired network design for single image super-resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
- Single image super-resolution, as a high dimensional structured prediction problem, aims to characterize fine- grain information given a low-resolution sample. Recent advances in convolutional neural networks are introduced into super-resolution and push forward progress in this field. Current studies have achieved impressive perfor- mance by manually designing deep residual neural net- works but overly relies on practical experience. In this paper, we propose to adopt an ordinary differential equa- tion (ODE)-inspired design scheme for single image super- resolution, which have brought us a new understanding of ResNet in classification problems. Not only is it in- terpretable for super-resolution but it provides a reliable guideline on network designs. By casting the numerical schemes in ODE as blueprints, we derive two types of net- work structures: LF-block and RK-block, which correspond to the Leapfrog method and Runge-Kutta method in numeri- cal ordinary differential equations. We evaluate our models on benchmark datasets, and the results show that our meth- ods surpass the state-of-the-arts while keeping comparable parameters and operations.
## ECCV 2019
### 1.Zhang, Yulun, et al. "Image super-resolution using very deep residual channel attention networks." Proceedings of the European conference on computer vision (ECCV). 2018.
- Convolutional neural network (CNN) depth is of crucial im- portance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low- resolution inputs and features contain abundant low-frequency informa- tion, which is treated equally across channels, hence hindering the rep- resentational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connec- tions. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.
##  SR with GAN 2017-2018
### 1.Ledig, Christian, et al. "Photo-realistic single image super-resolution using a generative adversarial network." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
### 2.Wang, Xintao, et al. "Esrgan: Enhanced super-resolution generative adversarial networks." Proceedings of the European conference on computer vision (ECCV) workshops. 2018.
### 3.Bulat, Adrian, Jing Yang, and Georgios Tzimiropoulos. "To learn image super-resolution, use a gan to learn how to do image degradation first." Proceedings of the European conference on computer vision (ECCV). 2018.
### 4.Yuan, Yuan, et al. "Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018.
### 5.Park, Seong-Jin, et al. "Srfeat: Single image super-resolution with feature discrimination." Proceedings of the European Conference on Computer Vision (ECCV). 2018.
### 6.Shocher, Assaf, Nadav Cohen, and Michal Irani. "“zero-shot” super-resolution using deep internal learning." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.

